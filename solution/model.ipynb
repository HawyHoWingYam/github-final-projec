{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_df = pd.read_csv('../processed_data/train_processed_with_dummies.csv')\n",
    "test_df = pd.read_csv('../processed_data/valid_processed_with_dummies.csv')\n",
    "\n",
    "train_X, train_y = train_df.iloc[1:, 0:-1], train_df.iloc[1:, -1:]\n",
    "train_X = torch.Tensor(train_X.to_numpy(dtype=float))\n",
    "train_y = torch.Tensor(train_y.to_numpy(dtype=float))\n",
    "\n",
    "test_X, test_y = test_df.iloc[:, 0:-1], test_df.iloc[:, -1:]\n",
    "test_X = torch.Tensor(test_X.to_numpy(dtype=float))\n",
    "test_y = torch.Tensor(test_y.to_numpy(dtype=float))\n",
    "\n",
    "train = TensorDataset(train_X, train_y)\n",
    "train_iter = DataLoader(train, batch_size = 32)\n",
    "test = TensorDataset(test_X, test_y)\n",
    "test_iter = DataLoader(test, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Model\n",
    "features = train_X.shape[1]\n",
    "\n",
    "regression = nn.Sequential(nn.Linear(features, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layer Proceptron\n",
    "MLP = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(features, 256), nn.ReLU(),\n",
    "                    nn.Linear(256, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(net, train_iter, epochs, loss, lr, device):\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for _,(X ,y) in enumerate(train_iter):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        log_rmse = torch.sqrt(loss(torch.log(y_hat),\n",
    "                           torch.log(y)))\n",
    "        print(f'epoch: {epoch}: loss: {log_rmse.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0: loss: 3.5894737243652344\n",
      "epoch: 1: loss: 2.9247424602508545\n",
      "epoch: 2: loss: 2.5380711555480957\n",
      "epoch: 3: loss: 2.266207456588745\n",
      "epoch: 4: loss: 2.057600498199463\n",
      "epoch: 5: loss: 1.8892208337783813\n",
      "epoch: 6: loss: 1.748758316040039\n",
      "epoch: 7: loss: 1.6288528442382812\n",
      "epoch: 8: loss: 1.5247514247894287\n",
      "epoch: 9: loss: 1.4332029819488525\n",
      "epoch: 10: loss: 1.3518857955932617\n",
      "epoch: 11: loss: 1.2790825366973877\n",
      "epoch: 12: loss: 1.2134844064712524\n",
      "epoch: 13: loss: 1.154070496559143\n",
      "epoch: 14: loss: 1.100027084350586\n",
      "epoch: 15: loss: 1.0506930351257324\n",
      "epoch: 16: loss: 1.0055229663848877\n",
      "epoch: 17: loss: 0.9640594124794006\n",
      "epoch: 18: loss: 0.9259145855903625\n",
      "epoch: 19: loss: 0.8907546997070312\n",
      "epoch: 20: loss: 0.8582903742790222\n",
      "epoch: 21: loss: 0.8282676935195923\n",
      "epoch: 22: loss: 0.8004627823829651\n",
      "epoch: 23: loss: 0.774675726890564\n",
      "epoch: 24: loss: 0.750728964805603\n",
      "epoch: 25: loss: 0.728461742401123\n",
      "epoch: 26: loss: 0.7077298760414124\n",
      "epoch: 27: loss: 0.6884030103683472\n",
      "epoch: 28: loss: 0.6703625917434692\n",
      "epoch: 29: loss: 0.653501570224762\n",
      "epoch: 30: loss: 0.6377223134040833\n",
      "epoch: 31: loss: 0.6229362487792969\n",
      "epoch: 32: loss: 0.6090626120567322\n",
      "epoch: 33: loss: 0.5960283279418945\n",
      "epoch: 34: loss: 0.5837662816047668\n",
      "epoch: 35: loss: 0.5722161531448364\n",
      "epoch: 36: loss: 0.5613222122192383\n",
      "epoch: 37: loss: 0.551034152507782\n",
      "epoch: 38: loss: 0.5413059592247009\n",
      "epoch: 39: loss: 0.532095730304718\n",
      "epoch: 40: loss: 0.5233651399612427\n",
      "epoch: 41: loss: 0.5150792002677917\n",
      "epoch: 42: loss: 0.5072064995765686\n",
      "epoch: 43: loss: 0.49971747398376465\n",
      "epoch: 44: loss: 0.4925857186317444\n",
      "epoch: 45: loss: 0.4857868552207947\n",
      "epoch: 46: loss: 0.47929835319519043\n",
      "epoch: 47: loss: 0.47310012578964233\n",
      "epoch: 48: loss: 0.467173308134079\n",
      "epoch: 49: loss: 0.4615004062652588\n",
      "epoch: 50: loss: 0.45606568455696106\n",
      "epoch: 51: loss: 0.4508547782897949\n",
      "epoch: 52: loss: 0.4458540380001068\n",
      "epoch: 53: loss: 0.44105100631713867\n",
      "epoch: 54: loss: 0.43643441796302795\n",
      "epoch: 55: loss: 0.43199339509010315\n",
      "epoch: 56: loss: 0.42771878838539124\n",
      "epoch: 57: loss: 0.42360082268714905\n",
      "epoch: 58: loss: 0.4196314811706543\n",
      "epoch: 59: loss: 0.41580283641815186\n",
      "epoch: 60: loss: 0.4121076464653015\n",
      "epoch: 61: loss: 0.4085389971733093\n",
      "epoch: 62: loss: 0.4050908386707306\n",
      "epoch: 63: loss: 0.401757150888443\n",
      "epoch: 64: loss: 0.39853259921073914\n",
      "epoch: 65: loss: 0.3954119086265564\n",
      "epoch: 66: loss: 0.39239034056663513\n",
      "epoch: 67: loss: 0.3894633650779724\n",
      "epoch: 68: loss: 0.3866269886493683\n",
      "epoch: 69: loss: 0.38387686014175415\n",
      "epoch: 70: loss: 0.3812094032764435\n",
      "epoch: 71: loss: 0.3786211907863617\n",
      "epoch: 72: loss: 0.37610915303230286\n",
      "epoch: 73: loss: 0.3736698031425476\n",
      "epoch: 74: loss: 0.37130028009414673\n",
      "epoch: 75: loss: 0.3689980208873749\n",
      "epoch: 76: loss: 0.3667601943016052\n",
      "epoch: 77: loss: 0.3645845353603363\n",
      "epoch: 78: loss: 0.36246833205223083\n",
      "epoch: 79: loss: 0.3604097068309784\n",
      "epoch: 80: loss: 0.3584064245223999\n",
      "epoch: 81: loss: 0.3564564883708954\n",
      "epoch: 82: loss: 0.3545578718185425\n",
      "epoch: 83: loss: 0.35270896553993225\n",
      "epoch: 84: loss: 0.35090774297714233\n",
      "epoch: 85: loss: 0.34915295243263245\n",
      "epoch: 86: loss: 0.3474425971508026\n",
      "epoch: 87: loss: 0.34577521681785583\n",
      "epoch: 88: loss: 0.34414979815483093\n",
      "epoch: 89: loss: 0.34256449341773987\n",
      "epoch: 90: loss: 0.3410181403160095\n",
      "epoch: 91: loss: 0.3395095467567444\n",
      "epoch: 92: loss: 0.33803731203079224\n",
      "epoch: 93: loss: 0.33660057187080383\n",
      "epoch: 94: loss: 0.33519798517227173\n",
      "epoch: 95: loss: 0.33382856845855713\n",
      "epoch: 96: loss: 0.3324911296367645\n",
      "epoch: 97: loss: 0.3311847746372223\n",
      "epoch: 98: loss: 0.3299086093902588\n",
      "epoch: 99: loss: 0.3286617696285248\n"
     ]
    }
   ],
   "source": [
    "epochs, lr = 100, 0.01\n",
    "loss = nn.MSELoss()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train(regression, train_iter, epochs, loss, lr, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0: loss: 0.4524247944355011\n",
      "epoch: 1: loss: 0.36306145787239075\n",
      "epoch: 2: loss: 0.3155585825443268\n",
      "epoch: 3: loss: 0.2929476499557495\n",
      "epoch: 4: loss: 0.2790246307849884\n",
      "epoch: 5: loss: 0.26892992854118347\n",
      "epoch: 6: loss: 0.2608376741409302\n",
      "epoch: 7: loss: 0.2542244493961334\n",
      "epoch: 8: loss: 0.2488587200641632\n",
      "epoch: 9: loss: 0.24453699588775635\n",
      "epoch: 10: loss: 0.2411893904209137\n",
      "epoch: 11: loss: 0.23869915306568146\n",
      "epoch: 12: loss: 0.23686587810516357\n",
      "epoch: 13: loss: 0.23560167849063873\n",
      "epoch: 14: loss: 0.2348906695842743\n",
      "epoch: 15: loss: 0.23457086086273193\n",
      "epoch: 16: loss: 0.23446178436279297\n",
      "epoch: 17: loss: 0.2344427853822708\n",
      "epoch: 18: loss: 0.23448419570922852\n",
      "epoch: 19: loss: 0.23433725535869598\n",
      "epoch: 20: loss: 0.23455367982387543\n",
      "epoch: 21: loss: 0.23501937091350555\n",
      "epoch: 22: loss: 0.23519417643547058\n",
      "epoch: 23: loss: 0.23568545281887054\n",
      "epoch: 24: loss: 0.23623397946357727\n",
      "epoch: 25: loss: 0.2361423671245575\n",
      "epoch: 26: loss: 0.2365499883890152\n",
      "epoch: 27: loss: 0.23648756742477417\n",
      "epoch: 28: loss: 0.23624680936336517\n",
      "epoch: 29: loss: 0.23634037375450134\n",
      "epoch: 30: loss: 0.23635421693325043\n",
      "epoch: 31: loss: 0.23604078590869904\n",
      "epoch: 32: loss: 0.23599384725093842\n",
      "epoch: 33: loss: 0.2359100580215454\n",
      "epoch: 34: loss: 0.2355741709470749\n",
      "epoch: 35: loss: 0.23557637631893158\n",
      "epoch: 36: loss: 0.23545576632022858\n",
      "epoch: 37: loss: 0.23474444448947906\n",
      "epoch: 38: loss: 0.23486557602882385\n",
      "epoch: 39: loss: 0.23435692489147186\n",
      "epoch: 40: loss: 0.23424401879310608\n",
      "epoch: 41: loss: 0.23365655541419983\n",
      "epoch: 42: loss: 0.23385310173034668\n",
      "epoch: 43: loss: 0.23243123292922974\n",
      "epoch: 44: loss: 0.23264457285404205\n",
      "epoch: 45: loss: 0.2326347529888153\n",
      "epoch: 46: loss: 0.23154932260513306\n",
      "epoch: 47: loss: 0.23122213780879974\n",
      "epoch: 48: loss: 0.23048700392246246\n",
      "epoch: 49: loss: 0.23069548606872559\n",
      "epoch: 50: loss: 0.23036661744117737\n",
      "epoch: 51: loss: 0.22929446399211884\n",
      "epoch: 52: loss: 0.22895942628383636\n",
      "epoch: 53: loss: 0.22891712188720703\n",
      "epoch: 54: loss: 0.2279190719127655\n",
      "epoch: 55: loss: 0.2278727889060974\n",
      "epoch: 56: loss: 0.22727902233600616\n",
      "epoch: 57: loss: 0.22743913531303406\n",
      "epoch: 58: loss: 0.22618773579597473\n",
      "epoch: 59: loss: 0.22629941999912262\n",
      "epoch: 60: loss: 0.22575971484184265\n",
      "epoch: 61: loss: 0.2251892238855362\n",
      "epoch: 62: loss: 0.22550439834594727\n",
      "epoch: 63: loss: 0.22462862730026245\n",
      "epoch: 64: loss: 0.22431516647338867\n",
      "epoch: 65: loss: 0.22421932220458984\n",
      "epoch: 66: loss: 0.22344253957271576\n",
      "epoch: 67: loss: 0.22319333255290985\n",
      "epoch: 68: loss: 0.22301216423511505\n",
      "epoch: 69: loss: 0.22276508808135986\n",
      "epoch: 70: loss: 0.222441166639328\n",
      "epoch: 71: loss: 0.2221192568540573\n",
      "epoch: 72: loss: 0.22188769280910492\n",
      "epoch: 73: loss: 0.2216859608888626\n",
      "epoch: 74: loss: 0.22109557688236237\n",
      "epoch: 75: loss: 0.22145235538482666\n",
      "epoch: 76: loss: 0.22051431238651276\n",
      "epoch: 77: loss: 0.2211037278175354\n",
      "epoch: 78: loss: 0.2198995053768158\n",
      "epoch: 79: loss: 0.22013385593891144\n",
      "epoch: 80: loss: 0.2197962999343872\n",
      "epoch: 81: loss: 0.21965432167053223\n",
      "epoch: 82: loss: 0.21932724118232727\n",
      "epoch: 83: loss: 0.21882790327072144\n",
      "epoch: 84: loss: 0.2187252789735794\n",
      "epoch: 85: loss: 0.21825772523880005\n",
      "epoch: 86: loss: 0.21824829280376434\n",
      "epoch: 87: loss: 0.217877596616745\n",
      "epoch: 88: loss: 0.21788538992404938\n",
      "epoch: 89: loss: 0.21765075623989105\n",
      "epoch: 90: loss: 0.21706324815750122\n",
      "epoch: 91: loss: 0.21698863804340363\n",
      "epoch: 92: loss: 0.21679939329624176\n",
      "epoch: 93: loss: 0.21657894551753998\n",
      "epoch: 94: loss: 0.2164900153875351\n",
      "epoch: 95: loss: 0.21580034494400024\n",
      "epoch: 96: loss: 0.21566151082515717\n",
      "epoch: 97: loss: 0.21588999032974243\n",
      "epoch: 98: loss: 0.21549047529697418\n",
      "epoch: 99: loss: 0.21525336802005768\n"
     ]
    }
   ],
   "source": [
    "train(MLP, train_iter, epochs, loss, lr, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "def valid(net, test_iter, device, loss):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        net = net.to(device)\n",
    "        l = 0\n",
    "        for X ,y in test_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_hat = net(X)\n",
    "            log_rmse = torch.sqrt(loss(torch.log(y_hat),\n",
    "                           torch.log(y)))\n",
    "            l = l + log_rmse.item()\n",
    "        \n",
    "    return l / len(list(test_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Validation Loss: 0.26052657089063097\n",
      "MLP Validation Loss: 0.17533192144972937\n"
     ]
    }
   ],
   "source": [
    "regression_valid = valid(regression, test_iter, device, loss)\n",
    "MLP_valid = valid(MLP, test_iter, device, loss)\n",
    "\n",
    "print(f'Regression Validation Loss: {regression_valid}')\n",
    "print(f'MLP Validation Loss: {MLP_valid}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
